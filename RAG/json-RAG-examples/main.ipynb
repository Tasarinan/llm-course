{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TAMANG\\Documents\\GitHub\\json-vector-rag-pipeline\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "file_path = \"FAQ.json\"\n",
    "\n",
    "# Load the JSON File as Documents\n",
    "loader = JSONLoader(file_path=file_path, jq_schema=\".[]\", text_content=False)\n",
    "documents = loader.load()\n",
    "\n",
    "# Initialize an Open-Source Embedding Model (using SentenceTransformers)\n",
    "model_name = \"all-MiniLM-L6-v2\"  # Lightweight and free\n",
    "# Create a HuggingFaceEmbeddings instance which internally uses SentenceTransformers\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "# Create a FAISS Vector Store from the Documents\n",
    "faiss_db = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "# (Optional) Persist the FAISS index locally for later use.\n",
    "faiss_db.save_local(\"data/faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='{\"question\": \"What is the product of the week?\", \"answer\": \"Our Upa Flowers are the product of the week.\"}' metadata={'source': 'C:\\\\Users\\\\TAMANG\\\\Documents\\\\GitHub\\\\json-vector-rag-pipeline\\\\FAQ.json', 'seq_num': 1}\n",
      "page_content='{\"question\": \"Do you offer subscription services?\", \"answer\": \"Yes, we offer weekly, bi-weekly, and monthly flower subscription services. You can choose the frequency, duration, and type of flowers you'd like to receive regularly.\"}' metadata={'source': 'C:\\\\Users\\\\TAMANG\\\\Documents\\\\GitHub\\\\json-vector-rag-pipeline\\\\FAQ.json', 'seq_num': 9}\n",
      "page_content='{\"question\": \"How far in advance should I order for special occasions like Valentine's Day?\", \"answer\": \"For major holidays like Valentine's Day, Mother's Day, or Christmas, we recommend placing your order at least 1-2 weeks in advance to ensure availability and timely delivery.\"}' metadata={'source': 'C:\\\\Users\\\\TAMANG\\\\Documents\\\\GitHub\\\\json-vector-rag-pipeline\\\\FAQ.json', 'seq_num': 15}\n",
      "page_content='{\"question\": \"What are your delivery options?\", \"answer\": \"We offer same-day delivery for orders placed before noon, next-day delivery, and scheduled delivery for future dates. Delivery options may vary based on your location and the availability of the flowers you've selected.\"}' metadata={'source': 'C:\\\\Users\\\\TAMANG\\\\Documents\\\\GitHub\\\\json-vector-rag-pipeline\\\\FAQ.json', 'seq_num': 4}\n"
     ]
    }
   ],
   "source": [
    "# Example query text\n",
    "query = \"what is the product of the week?\"\n",
    "\n",
    "# Perform a similarity search\n",
    "results = faiss_db.similarity_search(query, )\n",
    "\n",
    "# Print out the results\n",
    "for result in results:\n",
    "    print(result)  # or result.metadata, depending on what you stored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If the Computer is restarted then you can load local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"question\": \"What is the product of the week?\", \"answer\": \"Our Upa Flowers are the product of the week.\"}\n",
      "{\"question\": \"Do you offer subscription services?\", \"answer\": \"Yes, we offer weekly, bi-weekly, and monthly flower subscription services. You can choose the frequency, duration, and type of flowers you'd like to receive regularly.\"}\n",
      "{\"question\": \"How far in advance should I order for special occasions like Valentine's Day?\", \"answer\": \"For major holidays like Valentine's Day, Mother's Day, or Christmas, we recommend placing your order at least 1-2 weeks in advance to ensure availability and timely delivery.\"}\n",
      "{\"question\": \"What are your delivery options?\", \"answer\": \"We offer same-day delivery for orders placed before noon, next-day delivery, and scheduled delivery for future dates. Delivery options may vary based on your location and the availability of the flowers you've selected.\"}\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"all-MiniLM-L6-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "# Load the persisted FAISS index with dangerous deserialization allowed\n",
    "faiss_db = FAISS.load_local(\"data/faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "query = \"what is the product of the week?\"\n",
    "results = faiss_db.similarity_search(query)\n",
    "\n",
    "for result in results:\n",
    "    print(result.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to add other json files (i.e., new data) into this existing vector db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Define paths for the new JSON files\n",
    "new_files = [\n",
    "    \"privacy-policy.json\",\n",
    "    \"common-questions.json\"\n",
    "]\n",
    "\n",
    "# Initialize your embedding model (ensure it's the same as used before)\n",
    "model_name = \"all-MiniLM-L6-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "# Load the existing FAISS index from disk, allowing dangerous deserialization if needed\n",
    "faiss_db = FAISS.load_local(\"data/faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "# Loop over each new JSON file, load documents and add them to the index\n",
    "for file_path in new_files:\n",
    "    # Load the JSON file as documents\n",
    "    loader = JSONLoader(file_path=file_path, jq_schema=\".[]\", text_content=False)\n",
    "    new_documents = loader.load()\n",
    "    \n",
    "    # Add the new documents to the existing FAISS index\n",
    "    faiss_db.add_documents(new_documents)\n",
    "\n",
    "# Optionally, persist the updated FAISS index\n",
    "faiss_db.save_local(\"data/faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"all-MiniLM-L6-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "# Load the persisted FAISS index with dangerous deserialization allowed\n",
    "faiss_db = FAISS.load_local(\"data/faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "query = \"troubleshoot\"\n",
    "results = faiss_db.similarity_search(query)\n",
    "\n",
    "for result in results:\n",
    "    print(result.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "def query_faiss(user_query, index_path=\"data/faiss_index\", top_k=5):\n",
    "    \"\"\"\n",
    "    Given a user query, this function loads the persisted FAISS index,\n",
    "    performs a similarity search, and returns the top_k matching documents.\n",
    "    \n",
    "    Args:\n",
    "        user_query (str): The input query from the user.\n",
    "        index_path (str): The path where the FAISS index is saved.\n",
    "        top_k (int): Number of similar documents to return.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of the top_k matching document objects.\n",
    "    \"\"\"\n",
    "    # Initialize the embedding model (should match the one used to build the index)\n",
    "    model_name = \"all-MiniLM-L6-v2\"\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "    \n",
    "    # Load the persisted FAISS index (allow dangerous deserialization if needed)\n",
    "    faiss_db = FAISS.load_local(index_path, embeddings, allow_dangerous_deserialization=True)\n",
    "    \n",
    "    # Perform similarity search with the given query\n",
    "    results = faiss_db.similarity_search(user_query, k=top_k)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top matching documents:\n",
      "-----\n",
      "{\"question\": \"What types of flowers do you offer?\", \"answer\": \"We offer a wide variety of flowers including roses, lilies, tulips, sunflowers, orchids, and seasonal bouquets. Our selection changes regularly to ensure freshness and variety.\"}\n",
      "-----\n",
      "{\"question\": \"What if the flowers I want are out of season?\", \"answer\": \"If certain flowers are out of season, we offer similar alternatives or can create custom arrangements using in-season blooms that match your desired style and color scheme.\"}\n",
      "-----\n",
      "{\"question\": \"How do I ensure my flowers stay fresh?\", \"answer\": \"To keep your flowers fresh, change the water every 2-3 days, trim the stems at an angle, remove any leaves below the waterline, and keep the flowers away from direct sunlight and heat sources.\"}\n",
      "-----\n",
      "{\"question\": \"How do I know which flowers are appropriate for different occasions?\", \"answer\": \"Our website provides guidance on flower meanings and appropriate choices for various occasions. You can also chat with our customer service team for personalized recommendations.\"}\n",
      "-----\n",
      "{\"question\": \"Can I customize my bouquet?\", \"answer\": \"Yes, we offer customization options. You can choose specific flowers, colors, and arrangements. For special requests, please contact our customer service team.\"}\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    query = input(\"Enter your query: \")\n",
    "    similar_docs = query_faiss(query)\n",
    "    \n",
    "    print(\"\\nTop matching documents:\")\n",
    "    for doc in similar_docs:\n",
    "        print(\"-----\")\n",
    "        print(doc.page_content)  # or use doc.metadata if that's more appropriate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamlitCallbackHandler  # example callback for streaming output\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "# Load environment variables (make sure OPENAI_API_KEY is set)\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "def construct_rag_prompt(query, retrieved_docs):\n",
    "    \"\"\"\n",
    "    Build a prompt that includes retrieved document content and the user's query.\n",
    "    \"\"\"\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "    prompt = (\n",
    "        \"You are a helpful assistant that uses the following context to answer the question.\\n\\n\"\n",
    "        \"Context:\\n\"\n",
    "        f\"{context}\\n\\n\"\n",
    "        \"Question:\\n\"\n",
    "        f\"{query}\\n\\n\"\n",
    "        \"Note: If the 'Question' the user asks is not present in the 'Context', then you will respond with 'Unfortunately we do not have answer for that.\"\n",
    "    )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- RAG Prompt ---\n",
      "\n",
      "You are a helpful assistant that uses the following context to answer the question.\n",
      "\n",
      "Context:\n",
      "{\"question\": \"What is the product of the week?\", \"answer\": \"Our Upa Flowers are the product of the week.\"}\n",
      "\n",
      "{\"question\": \"What are your delivery options?\", \"answer\": \"We offer same-day delivery for orders placed before noon, next-day delivery, and scheduled delivery for future dates. Delivery options may vary based on your location and the availability of the flowers you've selected.\"}\n",
      "\n",
      "{\"question\": \"Do you offer subscription services?\", \"answer\": \"Yes, we offer weekly, bi-weekly, and monthly flower subscription services. You can choose the frequency, duration, and type of flowers you'd like to receive regularly.\"}\n",
      "\n",
      "Question:\n",
      "what is the product of the day?\n",
      "\n",
      "Note: If the 'Question' the user asks is not present in the 'Context', then you will respond with 'Unfortunately we do not have answer for that.\n",
      "\n",
      "\n",
      "--- Streaming Response ---\n",
      "\n",
      "Unfortunately we do not have answer for that."
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Get user's query\n",
    "user_query = input(\"Enter your query: \")\n",
    "\n",
    "# Step 1: Retrieve similar documents using the FAISS index\n",
    "retrieved_docs = query_faiss(user_query, index_path=\"data/faiss_index\", top_k=3)\n",
    "\n",
    "# Step 2: Construct the RAG prompt\n",
    "prompt = construct_rag_prompt(user_query, retrieved_docs)\n",
    "print(\"\\n\\n--- RAG Prompt ---\\n\")\n",
    "print(prompt)\n",
    "print(\"\\n\\n--- Streaming Response ---\\n\")\n",
    "\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions using provided context.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    stream=True,  # Enable streaming mode\n",
    "    temperature=0.7,\n",
    "    max_tokens=1000,\n",
    ")\n",
    "\n",
    "answer = \"\"\n",
    "\n",
    "for chunk in stream:\n",
    "    print(chunk.choices[0].delta.content or \"\", end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
