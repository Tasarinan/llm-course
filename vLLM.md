[LLM Compressor is Here: Faster Inference with vLLM](https://neuralmagic.com/blog/llm-compressor-is-here-faster-inference-with-vllm/)

[How to deploy DeepSeek V2 Lite models using vLLM](https://www.53ai.com/news/finetuning/2024090663471.html)

[Offline Inference Chat](https://docs.vllm.ai/en/v0.6.4/getting_started/examples/offline_inference_chat.html)


[FP8 KV Cache](https://docs.vllm.ai/en/latest/getting_started/examples/fp8.html)
